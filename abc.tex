\section{Artificial Bee Colonyアルゴリズム(ABC)}
\subsection{概要}
郡知能とは個々での単純な行動が，集団としては高度な振る舞いをすることを模した最適化アルゴリズムである．
そしてABCは2005年にkaraboraによって提案された郡知能の一つである\cite{abc}．
ABCは蜜蜂の採餌行動から着想を得ており，収穫蜂，追従蜂，偵察蜂の３種類の蜂によって探索を行う．
収穫蜂はすべての食物源に対して探索を行い，追従蜂は評価値の高い食物源を優先して探索を行なう．
追従蜂の探索によって評価値の高い食物源の近傍を局所的に探索することができる．
収穫蜂と追従蜂による探索で改善されなかった食物源は偵察蜂によってランダムな新しい食物源に置き換えられる．
偵察蜂によって局所最適解からの脱出が可能になり，探索範囲を広げることができる．
これら３種類の蜂による探索を繰り返すことで，より良い解を求めていく．
\subsection{探索手順}
ABCによって，(\ref{problem})式のような目的関数を$f(\boldsymbol{x})$を最小とするD次元の$\boldsymbol{x}$を求める問題を解くことを考える．
\begin{align}
    \label{problem}
\text{min}.~f(\boldsymbol{x}), \text{~~subject to } \boldsymbol{x} \in \mathbb{R} ^D
\end{align}
ABCでは食物源の数$n$と探索上限回数limitの２つのパラメータをあらかじめ設定する必要がある．
食物源の数を$n$とすると，収穫蜂，追従蜂の数はそれぞれ$n$ずつとなる．
limitは偵察蜂フェーズで使用する食物源の探索回数の上限である．
この値を小さくするとより広範囲の探索に，大きくするとより局所的な探索を行う．
各食物源に番号を振り分け，$i$番目の食物源を$\boldsymbol{x_{i}}(i = 1,...,n)$とすると
$\boldsymbol{x_{i}}$の評価値$\mathrm{fit}(\boldsymbol{x_{i}})$は
(\ref{eva})式の評価関数により求められる．
\begin{align}
    \label{eva} 
    \mathrm{fit}(\boldsymbol{x_{i}}) =
    \begin{cases}
    \dfrac{1}{1+f(\boldsymbol{x_{i}})} & \text{if } f(\boldsymbol{x_{i}}) \geq 0   \\
    \left\lvert1+f(\boldsymbol{x_{i}})\right\rvert & otherwise
    \end{cases}
\end{align}
\subsubsection*{step 0 準備}
ABCにおけるパラメータである食物源の数$n$と探索上限回数limitの設定を行う．次に終了条件の設定を行なう．
終了条件としては，アルゴリズムの反復回数が最大反復回数を超えたとき，
最良の食物源の評価値もしくは目的関数値が一定値を超えたときなどがある．
\subsubsection*{step 1 初期化}
初期化では，(\ref{init})式によって各食物源を各成分の範囲内でランダムに生成する．
さらに各食物源の探索回数$trial_i$を0に初期化する．
$x_{ij}$は$\boldsymbol{x_{i}}$の$j(j=1,...,D)$番目の成分， 
$x_{\min_j}$，$x_{\max_j}$はそれぞれ食物源の各成分の最小値，最大値である．また，$\mathrm{rand}$は一様乱数を表す． 
\begin{align}
    x_{ij} = x_{\min_j} + \mathrm{rand}[0,1](x_{\max_j}-x_{\min_j})\label{init}
\end{align}
初期化された食物源の中で最も評価値が高いものを$\boldsymbol{x}_{best}$とし，
その食物源のインデックスを$i_{best}$とする
\subsubsection*{step 2 収穫蜂フェーズ}
収穫蜂フェーズでは収穫蜂が食物源の近傍を探索する．
このときの更新式は（\ref{harvest}）式のようになる．
ここで$j，k(k\neq i)$はランダムに選択される．
\begin{align}
v_{ij} = x_{ij} + \mathrm{rand}[-1,1](x_{ij}-x_{kj})\label{harvest}
\end{align}
ここで$v_{ij}$と$x_{ij}$の評価値を比較し，$v_{ij}$が$x_{ij}$よりも優れていたら$x_{ij}$を$v_{ij}$に置き換え，
$trial_i$を0にリセットする．
そうでなければ，$trial_i$を1増やす．
\subsubsection*{step 3 追従蜂フェーズ}
追従蜂フェーズでは収穫蜂フェーズによって更新された食物源の評価値に基づいて，
ルーレット選択を行い更新個体を選択する．
食物源$x_i$の選択確率$p_i$は(\ref{roulette})式のようになる．
そのため，評価値が高い食物源ほど選択確率が高く，評価値が低い食物源は選択確率が低くなる．
食物源の更新は収穫蜂フェーズと同様に行われる．
この操作を$n$回行う．
\begin{align}
    p_i = \dfrac{\mathrm{fit}(\boldsymbol{x_{i}})}{\sum_{n}\mathrm{fit}(\boldsymbol{x_{j}})}\label{roulette}
\end{align}
\subsubsection*{step 4 偵察蜂フェーズ}
偵察蜂フェーズでは，trialの値が探索打ち切り回数limitに達した
食物源を(\ref{init})式によりランダムに生成した新たな解に置換する． 
置換した食物源のtrialを0にリセットする．
trialの値がlimitに達した
食物源がなければ何も行わない．
\subsubsection*{step 5 終了判定}
各食物源の評価値$\mathrm{fit}(\boldsymbol{x_{i}})$と，$\mathrm{fit}(\boldsymbol{x}_{best})$を比較し，
$\mathrm{fit}(\boldsymbol{x_{i}}) > \mathrm{fit}(\boldsymbol{x}_{best})$となる$\boldsymbol{x_{i}}$が存在する場合は
$\boldsymbol{x}_{best}$を更新する．
その後，終了条件を満たしている場合は最適解をその時の$\boldsymbol{x}_{best}$として探索を終了し，
そうでない場合は\textbf{step 2}に戻り，探索を続ける．
